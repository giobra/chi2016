We appreciate the effort that reviewers put in for our paper; we know
how demanding the CHI review process is.  However we find many
arguments being WRONG and the whole review being EXTREMELY WEAK.
This invalidates the conclusion of the primary reviewer. 

R1 asks for a comparisons with KLM: it could be done, but the outcome
would be similar to what we say with respect to CogTool (page 3, col
1, paragraph 3). SNIF-ACT applies to information websites: it is
questionable to use it on web apps. Furthermore it cannot be used when
only a model of the system is available.

We can restructure the Background section.

R1 says "...at the moment only analysis of the length of the
paths...". THIS IS WRONG: pages 6,7,8 discuss length & number of
paths, of detours, probability of hitting a minimal path, action
density.

We don't think a user study is needed to prove that MIGTool provides
useful data. First, the experiment is too complex. Many factors
influence completion time: paths, layout, size of interactors, labels,
task, domain, user skill, attitude, expertise, familiarity. These
cannot be factored out without negatively affecting ecological
validity. Second, any interesting conclusion will entail "if we keep
constant all the factors except for paths, shorter paths are
associated to faster times". But such a conclusion does not need an
experiment. MIGTool allows to factor our the paths component and draw
analytical insights that can be used to optimize THAT component of the
design. It produces data that are very sensitive to different
designs: hence suitable for a comparison. It's the basis of any
analytical engineering activity. Furthermore, some of our
interpretations of data obtained from email apps agree with Gmail's
success.

Fig2 was added to illustrate that graphs are very complex, and
therefore that they cannot be used to visually specify interaction
scenarios. We could easily rephrase those parts to make them clearer. 

R2 says that the inference "users more easily follow some error free
<<<<<<< HEAD
paths when there are 3 times as many" is not necessarily true. We
said "might follow", implying therefore a possibility.  WRONG COMMENT
=======
paths when there are 3 times as many" is not necessaryily true. We
said "might follow", implying therefore a possibility.  
>>>>>>> 5c9d5b13d22130b651727fb7ba7d2065baad26af

R2 says that efficiency is measured in time, not in # of steps. First,
the ISO/IEC 25060 defines it as "resources expended in relation to the
accuracy and completeness with which users achieve goals". Resources
could include user actions, gestures, screens being activated and the
like. Second, completion time is a function of # of steps. 

On page 9, col 2, para 4 we said "The method should not be used to
draw final usability conclusions, as it is devoid of any concerns
dealing with what is presented to users" (as acknowledged by R2). Thus
we don't understand R2's criticism based on Fitts' law.

R2 says that it is not true that "Gmail users have the lowest
probability to hit an order 0 path" (page 7, col 1, para 1). 
Based on the numbers and the assumption that all the rest is the same
and that paths are equiprobable, this is actually a valid conclusion
(see fig 5). 

R2 says that human behavior is not only probability. We agree, but we
never stated that. Analyses similar to ours were done by other
researchers, such as [32].

R2 comments about making predictions without considering labels in
UI. That is our point: we want to be able to make predictions also
when labels are not known, by using only the control part of the
design. It's wrong to say that we don't consider "the semantic of the
interface": we do, but not the semantics attached to labels and
renderings. There are even books on such a thing: [15,31].  We could
rephrase some sentence to make it clearer.

Thank you R2 for the suggested papers. 

R3 says that our first contribution is "modeling the UI according to
statecharts"; we never said that: page 2 col 1 para 2, point a) says
"using a UML state machine model to specify interaction scenarios",
where interaction scenarios are defined later on in page 4, col 1,
para 5.

StateWebCharts does not talk of metrics.  SCXML is a standard for
representing statecharts. We opted instead for UML state machines. R3
does not like UML, or there are other more fundamental reasons?

R2 says that representing user traces in a graph is not new. We never
claimed that. Nor did we do anything with "capturing" user traces, as
R3 is saying. We use models to generate a compact representation of
*possible* user traces, and compute results based on models alone, not
actual data. In this way, MIGTool can be used in early development
stages.

R3 says that one way to compute metrics is to define them as a
distance in a metric space. It is unfair to ask authors to define
exactly the metrics that a reviewer has in mind. 

R3 comments on how our data could be used to draw usability
conclusions, similar to those deriving from Pirolli's Info Scent. Info
Scent applies to information web sites, as opposed to web
apps. Second, on page 9 col 2 para 4 we say how CogTool analysts could
be informed by data produced by MIGTool.


